{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yi Xiao(xiaoyiyi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data exploration and preparation\n",
    "##### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('train-balanced-sarcasm.csv')\n",
    "data = data.drop(columns=['author', 'subreddit', 'date','created_utc'])\n",
    "data.dropna(subset=['comment'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    505405\n",
       "1    505368\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is very balanced dataset\n",
    "data['label'].value_counts()\n",
    "#we find this is a very balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>I don't pay attention to her, but as long as s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>do you find ariana grande sexy ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Trick or treating in general is just weird...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>What's your weird or unsettling Trick or Treat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Blade Mastery+Masamune or GTFO!</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Probably Sephiroth. I refuse to taint his grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>You don't have to, you have a good build, buy ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>What to upgrade? I have $500 to spend (mainly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>I would love to see him at lolla.</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Probably count Kanye out Since the rest of his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>I think a significant amount would be against ...</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>I bet if that money was poured into college de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>Damn I was hoping God was real</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>James Shields Will Not Exercise Opt-Out Clause</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>They have an agenda.</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>There's no time to worry about leaks when they...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>Great idea!</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Team Specific Threads No matter how many artic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>Ayy bb wassup, it makes a bit more sense in co...</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>Ill give you a hint. They commented in this th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>what the fuck</td>\n",
       "      <td>22</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Star Wars, easy. I'm not that bothered about I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>noted.</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>You're adorable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>because it's what really bothers him... and it...</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>He actually acts like a moody emo girl on twit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>why you fail me, my precious?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Clinton struggles to gain traction in Florida,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>Pre-Flashpoint Clark and Lois.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Is that the Older Clark and Older Lois?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>She hugs him back tightly, burying her head in...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>*Guilt roles over Mel, and he hesitates for a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>At this point they're so stable I could build ...</td>\n",
       "      <td>17</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>They will never get the stability of the 3DS.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>Conservatism as an ideology is for sure a reac...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>I still doubt that \"all conservatives stand fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>Maybe not control, but certainly that is evide...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Today Russian media tweeted out that Wikileaks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>Mine auto renewed without asking me the other ...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Thank you, Jagex, for being cool Jagex has alw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>466</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>467 A lovely way!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>Jesus is a FNAF fan confirmed</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Jesus got</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>This would make me cry.</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>\"You are like the end piece of bread in a loaf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>At first I thought it was instructions on fixi...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Found this under a box of porno mags well clea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>This guy, there's no way he isn't trolling, ri...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Who?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010743</th>\n",
       "      <td>1</td>\n",
       "      <td>ZOMG!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>07 - 31 - 2009 7+3+1 = 11 2+9=11 07312009 = 1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010744</th>\n",
       "      <td>1</td>\n",
       "      <td>Clearly the death sentence would have avoided ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Gay man falsely convicted of molesting childre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010745</th>\n",
       "      <td>1</td>\n",
       "      <td>Wow, that was quick.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Eric Holder is expected to soon name a prosecu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010746</th>\n",
       "      <td>1</td>\n",
       "      <td>del *.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Texan judge orders Microsoft to stop selling W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010747</th>\n",
       "      <td>1</td>\n",
       "      <td>I like the kid holding up the sign that says \"...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>More photos of protestors. Sigh... socialism.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010748</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes, and there's no such thing as mental illne...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>And my parents had a rough upbringing/backgrou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010749</th>\n",
       "      <td>1</td>\n",
       "      <td>Thank you, Glen Beck, Rush Limbaugh, Sean Hann...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>Barney Frank Confronts Woman At Townhall Compa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010750</th>\n",
       "      <td>1</td>\n",
       "      <td>you and your facts...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Bank robberies, even with a note, come with an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010751</th>\n",
       "      <td>0</td>\n",
       "      <td>so cool.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>olds not old's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010752</th>\n",
       "      <td>1</td>\n",
       "      <td>What fine, upstanding young gentlemen.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>White Adult, 2 teens charged with beating elde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010753</th>\n",
       "      <td>1</td>\n",
       "      <td>Good luck with that.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Guantanamo teenager will sue US - 12 years old...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010754</th>\n",
       "      <td>1</td>\n",
       "      <td>The real question is why God hasn't killed Bar...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>If even one person on Earth who claims to spea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010755</th>\n",
       "      <td>1</td>\n",
       "      <td>Women shouldn't lead men anyway... it's in the...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Read the comment section. The Glenn Beck fluff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010756</th>\n",
       "      <td>1</td>\n",
       "      <td>Being in a region that is that hot and being f...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Well, I guess they must like fat chicks over t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010757</th>\n",
       "      <td>1</td>\n",
       "      <td>but he's totally racist</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Ron Paul Deserves the Nobel Peace Prize: Here ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010758</th>\n",
       "      <td>1</td>\n",
       "      <td>Thank you unions.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>No wonder California is broke - The $350K Nurs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010759</th>\n",
       "      <td>1</td>\n",
       "      <td>Foxnews is the most accurate reporting service...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>Fox News: Firefox most popular search engine a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010760</th>\n",
       "      <td>1</td>\n",
       "      <td>OMG, WHAT'S NEXT, KISSES?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Teens Hug Each Other. EVERYBODY PANIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010761</th>\n",
       "      <td>1</td>\n",
       "      <td>nono, he'll go back to 1985 to stop the Syrian...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Right so I actually don't have much of a probl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010762</th>\n",
       "      <td>0</td>\n",
       "      <td>Who said I didn't have a big dick?</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>Some of us have big dicks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010763</th>\n",
       "      <td>1</td>\n",
       "      <td>forgot to add</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Columbine is ten years old this year. People m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010764</th>\n",
       "      <td>1</td>\n",
       "      <td>So *that's* why I can point my finger and have...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I told my mom i didn't like religious people. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010765</th>\n",
       "      <td>1</td>\n",
       "      <td>OH SWEET ANOTHER GUITAR HERO CLONE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Brutal Legend Screenshots Will Rock Your Face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010766</th>\n",
       "      <td>1</td>\n",
       "      <td>oh wow, I have never seen this before.</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>WHO WAS PHONE?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010767</th>\n",
       "      <td>1</td>\n",
       "      <td>:O</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US combat troops may stay in northern Iraq aft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010768</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm sure that Iran and N. Korea have the techn...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>No one is calling this an engineered pathogen,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010769</th>\n",
       "      <td>1</td>\n",
       "      <td>whatever you do, don't vote green!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>In a move typical of their recent do-nothing a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010770</th>\n",
       "      <td>1</td>\n",
       "      <td>Perhaps this is an atheist conspiracy to make ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Screw the Disabled--I've got to get to Church ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010771</th>\n",
       "      <td>1</td>\n",
       "      <td>The Slavs got their own country - it is called...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I've always been unsettled by that. I hear a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010772</th>\n",
       "      <td>1</td>\n",
       "      <td>values, as in capitalism .. there is good mone...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Why do the people who make our laws seem unabl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1010773 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                            comment  score  ups  \\\n",
       "0            0                                         NC and NH.      2   -1   \n",
       "1            0  You do know west teams play against west teams...     -4   -1   \n",
       "2            0  They were underdogs earlier today, but since G...      3    3   \n",
       "3            0  This meme isn't funny none of the \"new york ni...     -8   -1   \n",
       "4            0                    I could use one of those tools.      6   -1   \n",
       "5            0  I don't pay attention to her, but as long as s...      0    0   \n",
       "6            0      Trick or treating in general is just weird...      1   -1   \n",
       "7            0                    Blade Mastery+Masamune or GTFO!      2   -1   \n",
       "8            0  You don't have to, you have a good build, buy ...      1   -1   \n",
       "9            0                  I would love to see him at lolla.      2   -1   \n",
       "10           0  I think a significant amount would be against ...     92   92   \n",
       "11           0                     Damn I was hoping God was real     14   -1   \n",
       "12           0                               They have an agenda.      4   -1   \n",
       "13           0                                        Great idea!      1   -1   \n",
       "14           0  Ayy bb wassup, it makes a bit more sense in co...     29   29   \n",
       "15           0                                      what the fuck     22   -1   \n",
       "16           0                                             noted.      2   -1   \n",
       "17           0  because it's what really bothers him... and it...     15   -1   \n",
       "18           0                      why you fail me, my precious?      1    1   \n",
       "19           0                     Pre-Flashpoint Clark and Lois.      2    2   \n",
       "20           0  She hugs him back tightly, burying her head in...      1   -1   \n",
       "21           0  At this point they're so stable I could build ...     17   -1   \n",
       "22           0  Conservatism as an ideology is for sure a reac...      1   -1   \n",
       "23           0  Maybe not control, but certainly that is evide...      1   -1   \n",
       "24           0  Mine auto renewed without asking me the other ...      3   -1   \n",
       "25           0                                                466      5   -1   \n",
       "26           0                      Jesus is a FNAF fan confirmed      2   -1   \n",
       "27           0                            This would make me cry.      1   -1   \n",
       "28           0  At first I thought it was instructions on fixi...      1   -1   \n",
       "29           0  This guy, there's no way he isn't trolling, ri...      1   -1   \n",
       "...        ...                                                ...    ...  ...   \n",
       "1010743      1                                              ZOMG!      1    1   \n",
       "1010744      1  Clearly the death sentence would have avoided ...      2    2   \n",
       "1010745      1                               Wow, that was quick.      2    2   \n",
       "1010746      1                                          del *.xml      1    1   \n",
       "1010747      1  I like the kid holding up the sign that says \"...      0    0   \n",
       "1010748      1  Yes, and there's no such thing as mental illne...     -1   -1   \n",
       "1010749      1  Thank you, Glen Beck, Rush Limbaugh, Sean Hann...     12   12   \n",
       "1010750      1                              you and your facts...      2    2   \n",
       "1010751      0                                           so cool.      1    1   \n",
       "1010752      1             What fine, upstanding young gentlemen.      1    1   \n",
       "1010753      1                               Good luck with that.      1    1   \n",
       "1010754      1  The real question is why God hasn't killed Bar...      6    6   \n",
       "1010755      1  Women shouldn't lead men anyway... it's in the...      1    1   \n",
       "1010756      1  Being in a region that is that hot and being f...      2    2   \n",
       "1010757      1                            but he's totally racist      2    2   \n",
       "1010758      1                                  Thank you unions.      2    2   \n",
       "1010759      1  Foxnews is the most accurate reporting service...     10   10   \n",
       "1010760      1                          OMG, WHAT'S NEXT, KISSES?      0    0   \n",
       "1010761      1  nono, he'll go back to 1985 to stop the Syrian...      0    0   \n",
       "1010762      0                 Who said I didn't have a big dick?     -2   -2   \n",
       "1010763      1                                      forgot to add      1    1   \n",
       "1010764      1  So *that's* why I can point my finger and have...      1    1   \n",
       "1010765      1                 OH SWEET ANOTHER GUITAR HERO CLONE      1    1   \n",
       "1010766      1             oh wow, I have never seen this before.      3    3   \n",
       "1010767      1                                                 :O      1    1   \n",
       "1010768      1  I'm sure that Iran and N. Korea have the techn...      2    2   \n",
       "1010769      1                 whatever you do, don't vote green!      1    1   \n",
       "1010770      1  Perhaps this is an atheist conspiracy to make ...      1    1   \n",
       "1010771      1  The Slavs got their own country - it is called...      1    1   \n",
       "1010772      1  values, as in capitalism .. there is good mone...      2    2   \n",
       "\n",
       "         downs                                     parent_comment  \n",
       "0           -1  Yeah, I get that argument. At this point, I'd ...  \n",
       "1           -1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2            0                            They're favored to win.  \n",
       "3           -1                         deadass don't kill my buzz  \n",
       "4           -1  Yep can confirm I saw the tool they use for th...  \n",
       "5            0                   do you find ariana grande sexy ?  \n",
       "6           -1  What's your weird or unsettling Trick or Treat...  \n",
       "7           -1  Probably Sephiroth. I refuse to taint his grea...  \n",
       "8           -1  What to upgrade? I have $500 to spend (mainly ...  \n",
       "9           -1  Probably count Kanye out Since the rest of his...  \n",
       "10           0  I bet if that money was poured into college de...  \n",
       "11          -1     James Shields Will Not Exercise Opt-Out Clause  \n",
       "12          -1  There's no time to worry about leaks when they...  \n",
       "13          -1  Team Specific Threads No matter how many artic...  \n",
       "14           0  Ill give you a hint. They commented in this th...  \n",
       "15          -1  Star Wars, easy. I'm not that bothered about I...  \n",
       "16          -1                                   You're adorable.  \n",
       "17          -1  He actually acts like a moody emo girl on twit...  \n",
       "18           0  Clinton struggles to gain traction in Florida,...  \n",
       "19           0            Is that the Older Clark and Older Lois?  \n",
       "20          -1  *Guilt roles over Mel, and he hesitates for a ...  \n",
       "21          -1      They will never get the stability of the 3DS.  \n",
       "22          -1  I still doubt that \"all conservatives stand fo...  \n",
       "23          -1  Today Russian media tweeted out that Wikileaks...  \n",
       "24          -1  Thank you, Jagex, for being cool Jagex has alw...  \n",
       "25          -1                                  467 A lovely way!  \n",
       "26          -1                                          Jesus got  \n",
       "27          -1  \"You are like the end piece of bread in a loaf...  \n",
       "28          -1  Found this under a box of porno mags well clea...  \n",
       "29          -1                                               Who?  \n",
       "...        ...                                                ...  \n",
       "1010743      0   07 - 31 - 2009 7+3+1 = 11 2+9=11 07312009 = 1111  \n",
       "1010744      0  Gay man falsely convicted of molesting childre...  \n",
       "1010745      0  Eric Holder is expected to soon name a prosecu...  \n",
       "1010746      0  Texan judge orders Microsoft to stop selling W...  \n",
       "1010747      0  More photos of protestors. Sigh... socialism.....  \n",
       "1010748      0  And my parents had a rough upbringing/backgrou...  \n",
       "1010749      0  Barney Frank Confronts Woman At Townhall Compa...  \n",
       "1010750      0  Bank robberies, even with a note, come with an...  \n",
       "1010751      0                                     olds not old's  \n",
       "1010752      0  White Adult, 2 teens charged with beating elde...  \n",
       "1010753      0  Guantanamo teenager will sue US - 12 years old...  \n",
       "1010754      0  If even one person on Earth who claims to spea...  \n",
       "1010755      0  Read the comment section. The Glenn Beck fluff...  \n",
       "1010756      0  Well, I guess they must like fat chicks over t...  \n",
       "1010757      0  Ron Paul Deserves the Nobel Peace Prize: Here ...  \n",
       "1010758      0  No wonder California is broke - The $350K Nurs...  \n",
       "1010759      0  Fox News: Firefox most popular search engine a...  \n",
       "1010760      0              Teens Hug Each Other. EVERYBODY PANIC  \n",
       "1010761      0  Right so I actually don't have much of a probl...  \n",
       "1010762      0                         Some of us have big dicks.  \n",
       "1010763      0  Columbine is ten years old this year. People m...  \n",
       "1010764      0  I told my mom i didn't like religious people. ...  \n",
       "1010765      0      Brutal Legend Screenshots Will Rock Your Face  \n",
       "1010766      0                                     WHO WAS PHONE?  \n",
       "1010767      0  US combat troops may stay in northern Iraq aft...  \n",
       "1010768      0  No one is calling this an engineered pathogen,...  \n",
       "1010769      0  In a move typical of their recent do-nothing a...  \n",
       "1010770      0  Screw the Disabled--I've got to get to Church ...  \n",
       "1010771      0  I've always been unsettled by that. I hear a l...  \n",
       "1010772      0  Why do the people who make our laws seem unabl...  \n",
       "\n",
       "[1010773 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                            comment  score  ups  \\\n",
      "0      0                                         NC and NH.      2   -1   \n",
      "1      0  You do know west teams play against west teams...     -4   -1   \n",
      "2      0  They were underdogs earlier today, but since G...      3    3   \n",
      "3      0  This meme isn't funny none of the \"new york ni...     -8   -1   \n",
      "4      0                    I could use one of those tools.      6   -1   \n",
      "\n",
      "   downs                                     parent_comment  \n",
      "0     -1  Yeah, I get that argument. At this point, I'd ...  \n",
      "1     -1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
      "2      0                            They're favored to win.  \n",
      "3     -1                         deadass don't kill my buzz  \n",
      "4     -1  Yep can confirm I saw the tool they use for th...  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### split label and other information into x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.pop('label')\n",
    "x = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score : 0.500353\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "baseline_y = []\n",
    "for i in range(len(x)):\n",
    "    baseline_y.append(random.randint(0, 1))\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y, baseline_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. extract features\n",
    "#### feature1: we have existing features: score, ups, down, and we want to extract features from comment text and parent comment text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(x, y, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score : 0.519961\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train[['score','ups', 'downs']], y_train)\n",
    "y_pred = gnb.predict(X_test[['score','ups', 'downs']])\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### according to the accuracy score, looks like those three features are not very helpful features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature2:  bag of words， I think comments usually are not long, and context of words should not be in large scale, so 1gram - 3 gram shoud be ideal features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### feature 2.1.1 : only analyze comment text,  1-gram to 2-gram this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_f1, y_f1, y_train_f1, y_test_f1 = \\\n",
    "        train_test_split(x['comment'], y, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tf_idf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=50000, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "tf_idf_1 = TfidfVectorizer(ngram_range = (1, 2), max_features = 50000)\n",
    "logistic = LogisticRegression()\n",
    "# sklearn's pipeline\n",
    "tfidf_logitistic_pipeline_1 = Pipeline([('tf_idf', tf_idf_1), \n",
    "                                 ('logit', logistic)])\n",
    "tfidf_logitistic_pipeline_1.fit(X_f1, y_train_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score : 0.720939\n"
     ]
    }
   ],
   "source": [
    "y_pred_f1 = tfidf_logitistic_pipeline_1.predict(y_f1)\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test_f1, y_pred_f1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### feature 2.1.2 : only analyze comment text, use 1-gram to 3-gram this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_f2, y_f2, y_train_f2, y_test_f2 = \\\n",
    "        train_test_split(x['comment'], y, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469600     Starting to feel pretty fucking tired of all t...\n",
      "639137     It's like that label actually has no meaning b...\n",
      "240293     Mained Fiora - Reworked Mained AP Tristana - W...\n",
      "702254     Yeah lol that's right they wouldn't let black ...\n",
      "889040               No, he made the thread asking jokingly.\n",
      "118721     Probably - they flaunt a subscription button o...\n",
      "947749                                          So horrible!\n",
      "112289     You'd think someone would have foreseen this a...\n",
      "564578     Elasticity of driver material based on tempera...\n",
      "1020       I can't believe after ~30 comments I have yet ...\n",
      "44677                  nah dude OP is tryin to get followers\n",
      "43630      Remember, everyone: vote democrat if you want ...\n",
      "822869                             Download the NSA toolkit.\n",
      "950321                                  How original of you.\n",
      "995459     Better start sniffing coke and drinking a few ...\n",
      "1007434    It's like he invented a whole new type of chec...\n",
      "254789     I wasn't claiming anything - I'm simply trying...\n",
      "841394     I was on exactly 150 wins which would include ...\n",
      "207806       Muslims, obviously, with their free Sharia law.\n",
      "408072                         Yeah, it looks totally unreal\n",
      "877926     its on next weekend mate, il let you know though!\n",
      "265267         Klar weil Belgien ja auch die Luxemburg ist..\n",
      "926592     I'm not defending what he did, but to say that...\n",
      "15055                                             Thank you!\n",
      "909854     A world without biscuits and gravy is a world ...\n",
      "839234     But he makes more money than you making him mo...\n",
      "206571     that's not what he said, he said all Mexicans ...\n",
      "183643     I thought it was 6700000Ghz because of the k t...\n",
      "316896              I'll ban you for giving away my giveaway\n",
      "647063                              Probably used Bikko skin\n",
      "                                 ...                        \n",
      "327551     Do you have a \"do not disturb\" rule active whe...\n",
      "48963                                                     No\n",
      "357459     You don't even know how to calculate KDA, that...\n",
      "874910     Similar Poll: More and more people agreeing th...\n",
      "997641     It changes one important thing to me and many ...\n",
      "29432      Maybe we should elect that person instead of t...\n",
      "689953                 not enough explosions for Michael Bay\n",
      "406527                                     ...that's 3 words\n",
      "789093                            Because eugenics is great!\n",
      "379609                                             Thanks :D\n",
      "725625     Hiko was salty that he lost 16-1 against shahz...\n",
      "373253     Niners could become 2012 Colts, playing the QB...\n",
      "152999     Look at the insanely accurate bullet travel ti...\n",
      "313175     And now the game of \"What got ninja-nerfed/buf...\n",
      "986205     Yes, I too expect $8-12/hr employees to do exc...\n",
      "763885                                         They tend to.\n",
      "971903     HOW WOULD YOU KNOW IF MOTHERS DAUGHTER IS A LE...\n",
      "917238                                         MechaGodzilla\n",
      "152845           \"but... but... but... the founding fathers\"\n",
      "484758     He was also announced as \"forward John Scott\" ...\n",
      "436007               Announcers couldn't sound more excited.\n",
      "197804                                             Worth it.\n",
      "488291     I'd be down for this if it meant getting my se...\n",
      "298626     Yes, this mediocre list of achievements makes ...\n",
      "317888     No matter what people say to you Dardoch, Tyle...\n",
      "23988      Well the devs kind of stopped working on the o...\n",
      "408769            totally balanced, not game breaking at all\n",
      "688966     No amount of money can take that experience away.\n",
      "107936     But guys, you only think D2 was good because o...\n",
      "767875     BUT HE'S A CAREER THIRD LINER AND WE SHOULD DU...\n",
      "Name: comment, Length: 252694, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y_f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score : 0.721762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "tf_idf_2 = TfidfVectorizer(ngram_range = (1, 3), max_features = 50000)\n",
    "logistic = LogisticRegression()\n",
    "# sklearn's pipeline\n",
    "tfidf_logitistic_pipeline_2 = Pipeline([('tf_idf', tf_idf_2), \n",
    "                                 ('logit', logistic)])\n",
    "\n",
    "tfidf_logitistic_pipeline_2.fit(X_f2, y_train_f2)\n",
    "y_pred_f2 = tfidf_logitistic_pipeline_2.predict(y_f2)\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test_f2, y_pred_f2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the accuracy has been improved when we use 1-3 gram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score : 0.715248\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "svc = LinearSVC()\n",
    "tf_idf_2 = TfidfVectorizer(ngram_range = (1, 3), max_features = 50000)\n",
    "clf_pipeline_1 = Pipeline([('tf_idf', tf_idf_2), \n",
    "                                 ('svc', svc)])\n",
    "\n",
    "clf_pipeline_1.fit(X_f2, y_train_f2)\n",
    "y_pred_f2 = clf_pipeline_1.predict(y_f2)\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test_f2, y_pred_f2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score : 0.560164\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = tree.DecisionTreeClassifier(max_depth = 4)\n",
    "tf_idf_2 = TfidfVectorizer(ngram_range = (1, 3), max_features = 50000)\n",
    "clf_pipeline_2 = Pipeline([('tf_idf', tf_idf_2), \n",
    "                                 ('clf', clf)])\n",
    "\n",
    "clf_pipeline_2.fit(X_f2, y_train_f2)\n",
    "y_pred_f2 = clf_pipeline_2.predict(y_f2)\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test_f2, y_pred_f2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = GaussianNB()\n",
    "tf_idf_2 = TfidfVectorizer(ngram_range = (1, 3), max_features = 50000)\n",
    "clf_pipeline_3 = Pipeline([('tf_idf', tf_idf_2), \n",
    "                           ('to_dense', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),\n",
    "                                 ('clf', clf)])\n",
    "\n",
    "clf_pipeline_3.fit(X_f2, y_train_f2)\n",
    "y_pred_f2 = clf_pipeline_3.predict(y_f2)\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test_f2, y_pred_f2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### try to extend the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "tf_idf_3 = TfidfVectorizer(ngram_range = (2, 3), max_features = 50000)\n",
    "logistic = LogisticRegression()\n",
    "# sklearn's pipeline\n",
    "tfidf_logitistic_pipeline_3 = Pipeline([('tf_idf', tf_idf_3), \n",
    "                                 ('logit', logistic)])\n",
    "\n",
    "tfidf_logitistic_pipeline_3.fit(X_f2, y_train_f2)\n",
    "y_pred_f3 = tfidf_logitistic_pipeline_3.predict(y_f2)\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test_f2, y_pred_f3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "tf_idf_4 = TfidfVectorizer(ngram_range = (2, 4), max_features = 50000)\n",
    "logistic = LogisticRegression()\n",
    "# sklearn's pipeline\n",
    "tfidf_logitistic_pipeline_4 = Pipeline([('tf_idf', tf_idf_4), \n",
    "                                 ('logit', logistic)])\n",
    "\n",
    "tfidf_logitistic_pipeline_4.fit(X_f2, y_train_f2)\n",
    "y_pred_f4 = tfidf_logitistic_pipeline_4.predict(y_f2)\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test_f2, y_pred_f4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### feature 2.2 : only analyze parent comment text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_comments, y_comments, y_train_temp, y_test_temp = \\\n",
    "        train_test_split(x['parent_comment'], y, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "tf_idf = TfidfVectorizer(ngram_range = (1, 3), max_features = 50000)\n",
    "logistic = LogisticRegression()\n",
    "# sklearn's pipeline\n",
    "tfidf_logitistic_pipeline = Pipeline([('tf_idf', tf_idf), \n",
    "                                 ('logit', logistic)])\n",
    "tfidf_logitistic_pipeline.fit(X_comments, y_train_temp)\n",
    "y_pred = tfidf_logitistic_pipeline.predict(y_comments)\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test_temp, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it looks like parent_comments bag of words have quite light infuence on the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score : 0.574074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svc = LinearSVC()\n",
    "tf_idf_2 = TfidfVectorizer(ngram_range = (1, 3), max_features = 50000)\n",
    "clf_pipeline_1 = Pipeline([('tf_idf', tf_idf_2), \n",
    "                                 ('svc', svc)])\n",
    "\n",
    "clf_pipeline_1.fit(X_comments, y_train_temp)\n",
    "y_pred = clf_pipeline_1.predict(y_comments)\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test_temp, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score : 0.518184\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = tree.DecisionTreeClassifier(max_depth = 4)\n",
    "tf_idf_2 = TfidfVectorizer(ngram_range = (1, 3), max_features = 50000)\n",
    "clf_pipeline_2 = Pipeline([('tf_idf', tf_idf_2), \n",
    "                                 ('clf', clf)])\n",
    "\n",
    "clf_pipeline_2.fit(X_comments, y_train_temp)\n",
    "y_pred = clf_pipeline_2.predict(y_comments)\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test_temp, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### feature3. sentiment analysis\n",
    "\n",
    "as we know, sarcasm usually is saying something positive and then saying something negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### feature 3. 1 overall negative or overral postive, calculate out fraction of postive, fraction of negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### data preparation for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "ss_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "ss_list = []\n",
    "neg_frac_list = []\n",
    "pos_frac_list = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    try:\n",
    "        ss = ss_analyzer.polarity_scores(row['comment'])['compound']\n",
    "        tokens = nltk.word_tokenize(row['comment'])\n",
    "        neg_counts = 0\n",
    "        pos_counts = 0\n",
    "        for word in tokens:\n",
    "            ps = ss_analyzer.polarity_scores(word)\n",
    "            if ps[\"neg\"] == 1.0:\n",
    "                neg_counts += 1\n",
    "            elif ps[\"pos\"] == 1.0:\n",
    "                pos_counts += 1\n",
    "        neg_frac = neg_counts / len(tokens)\n",
    "        pos_frac = pos_counts / len(tokens)\n",
    "    except:\n",
    "        ss = 0\n",
    "        neg_frac = 0\n",
    "        pos_frac = 0\n",
    "    print(index)\n",
    "    ss_list.append(ss)\n",
    "    neg_frac_list.append(neg_frac)\n",
    "    pos_frac_list.append(pos_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neg_frac_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos_frac_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series(ss_list)\n",
    "data['ss'] = s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = pd.Series(neg_frac_list)\n",
    "data['neg_frac'] = s2\n",
    "s3 = pd.Series(pos_frac_list)\n",
    "data['pos_frac'] = s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv('data_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### feature 3.2 sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv('data_1.csv')\n",
    "y = data_1.pop('label')\n",
    "X = data_1\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression()\n",
    "from sklearn.metrics import accuracy_score\n",
    "logistic.fit(X_train[['ss']], y_train)\n",
    "y_pred = gnb.predict(X_test[['ss']])\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score : 0.538462\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression()\n",
    "from sklearn.metrics import accuracy_score\n",
    "logistic.fit(X_train[['pos_frac','pos_frac']], y_train)\n",
    "y_pred = logistic.predict(X_test[['pos_frac','pos_frac']])\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score : 0.538462\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC()\n",
    "from sklearn.metrics import accuracy_score\n",
    "svc.fit(X_train[['pos_frac','pos_frac']], y_train)\n",
    "y_pred = svc.predict(X_test[['pos_frac','pos_frac']])\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score : 0.545810\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth = 4)\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf.fit(X_train[['pos_frac','pos_frac']], y_train)\n",
    "y_pred = clf.predict(X_test[['pos_frac','pos_frac']])\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score : 0.498000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf.fit(X_train[['pos_frac','pos_frac']], y_train)\n",
    "y_pred = clf.predict(X_test[['pos_frac','pos_frac']])\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parent comment sentiment calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "ss_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "p_ss_list = []\n",
    "p_neg_frac_list = []\n",
    "p_pos_frac_list = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    try:\n",
    "        ss = ss_analyzer.polarity_scores(row['parent_comment'])['compound']\n",
    "        tokens = nltk.word_tokenize(row['parent_comment'])\n",
    "        neg_counts = 0\n",
    "        pos_counts = 0\n",
    "        for word in tokens:\n",
    "            ps = ss_analyzer.polarity_scores(word)\n",
    "            if ps[\"neg\"] == 1.0:\n",
    "                neg_counts += 1\n",
    "            elif ps[\"pos\"] == 1.0:\n",
    "                pos_counts += 1\n",
    "        neg_frac = neg_counts / len(tokens)\n",
    "        pos_frac = pos_counts / len(tokens)\n",
    "    except:\n",
    "        ss = 0\n",
    "        neg_frac = 0\n",
    "        pos_frac = 0\n",
    "    print(index)\n",
    "    p_ss_list.append(ss)\n",
    "    p_neg_frac_list.append(neg_frac)\n",
    "    p_pos_frac_list.append(pos_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_p = pd.Series(p_ss_list)\n",
    "data['p_ss'] = s1_p\n",
    "s2_p = pd.Series(p_neg_frac_list)\n",
    "data['p_neg_frac'] = s2_p\n",
    "s3_p = pd.Series(p_pos_frac_list)\n",
    "data['p_pos_frac'] = s3_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv('data_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_p = pd.Series(p_ss_list)\n",
    "data_1['p_ss'] = s1_p\n",
    "s2_p = pd.Series(p_neg_frac_list)\n",
    "data_1['p_neg_frac'] = s2_p\n",
    "s3_p = pd.Series(p_pos_frac_list)\n",
    "data_1['p_pos_frac'] = s3_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.to_csv(\"data_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = pd.read_csv('data_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_2.pop('label')\n",
    "X = data_2\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression()\n",
    "from sklearn.metrics import accuracy_score\n",
    "logistic.fit(X_train[['p_ss']], y_train)\n",
    "y_pred = logistic.predict(X_test[['p_ss']])\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score : 0.538462\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression()\n",
    "from sklearn.metrics import accuracy_score\n",
    "logistic.fit(X_train[['pos_frac','pos_frac']], y_train)\n",
    "y_pred = logistic.predict(X_test[['pos_frac','pos_frac']])\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score : 0.538462\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC()\n",
    "from sklearn.metrics import accuracy_score\n",
    "svc.fit(X_train[['pos_frac','pos_frac']], y_train)\n",
    "y_pred = svc.predict(X_test[['pos_frac','pos_frac']])\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score : 0.545810\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth = 4)\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf.fit(X_train[['pos_frac','pos_frac']], y_train)\n",
    "y_pred = clf.predict(X_test[['pos_frac','pos_frac']])\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score : 0.523377\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf.fit(X_train[['pos_frac','pos_frac']], y_train)\n",
    "y_pred = clf.predict(X_test[['pos_frac','pos_frac']])\n",
    "print(\"\\nAccuracy score : %f\" %(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
